---
layout: minimal
authors:
    - "thegreataxios"
date: 2025-11-19
title: "What is Agency?"
---

# What is Agency?

This article explores what "agency" actually means in the context of AI systems, why most AI being consumed today is not truly agentic, and why prompting is often confused with agency. Understanding the distinction between reactive AI systems and truly autonomous agents is crucial for building effective agentic applications.

## What Does Agency Actually Mean?

I've been building with AI tools and agentic systems for a while now, and I'm noticing that the term "agency" is being used pretty loosely. I think we might be losing clarity on what it actually means.

When I think about agency, I'm thinking about autonomous decision-making combined with goal pursuit, environmental interaction, and adaptation. It's not just about whether a system can call tools—it's about whether it can decide when to call tools, why to call them, and what to do when things go wrong.

The technical reality is that most systems being called "agents" today are really just prompt chains with function calling. They're reactive rather than proactive. They don't maintain state across sessions in a meaningful way, and they struggle to adapt their strategy when initial approaches fail.

I think agency requires a few key components:

- **Goal-setting**: the system needs to understand what it's trying to accomplish
- **Planning**: it needs to figure out how to get there
- **Execution**: it needs to actually do the work
- **Monitoring**: it needs to track progress and detect issues
- **Adaptation**: it needs to change course when things aren't working

Current AI systems excel at execution, but they struggle with the rest. The confusion, I think, comes from conflating "capability" with "autonomy." Just because a system can do something doesn't mean it has agency.

Maybe an unfair comparison, but think about it this way: a calculator can do math, but it doesn't have agency. It executes what you tell it to do. Most "agents" today are like really sophisticated calculators—they can do complex things, but they're still just executing instructions.

## Why Most AI Being Consumed is Not Agentic

I think most AI consumption today is still fundamentally prompt → response, just with more sophisticated prompting. [ChatGPT with plugins](https://platform.openai.com/docs/guides/tools), for example, still requires a human to orchestrate—it just has more tools available.

AutoGPT-style systems often get stuck in loops and need constant babysitting. Most "AI agents" I see in production are actually just automated workflows with LLM decision points, not truly autonomous systems.

There are real technical barriers to building truly agentic systems:

- **Cost**: true agency requires many API calls, which gets expensive fast
- **Reliability**: agents need to handle failures gracefully, and most don't
- **State management**: maintaining context and goals across long-running tasks is genuinely hard
- **Tool consistency**: agents need reliable, well-documented tools, and the ecosystem is still immature

I've built a few things I'd actually call agentic, and the difference is significant. The complexity goes up substantially, but so does the value when you get it right. I've been exploring this through building with [MCP servers](https://modelcontextprotocol.io) and [x402](https://x402.org) payments, and I'm seeing where real agency shines versus where it's overkill.

For example, I built a research agent that can autonomously explore a topic, verify information, and synthesize findings. That's agency—it sets its own sub-goals, monitors progress, and adapts when it hits dead ends. But I also built a simple API wrapper that formats responses. That doesn't need agency; it just needs to execute a function.

## The Confusion Between Prompting and Agency

I think the core confusion is that we see sophisticated prompting and think we're seeing agency. But prompting is the interface, not the capability. Just because you can prompt an LLM to do complex things doesn't mean it has agency.

LLMs are really good at following complex instructions. When you give them tools, they can appear autonomous. But they're still fundamentally reactive—they respond to prompts, they don't initiate action.

Better prompting doesn't equal more agency. You can prompt a system to appear agentic, but it's still just following instructions. The system doesn't "decide" to do something; it executes what you told it to do, even if that instruction is "figure out what to do."

The line between real agency and prompt engineering is blurry, but I think it matters for building scalable systems. Real agency means the system sets its own sub-goals, monitors progress, and adapts strategy. Prompt engineering means a human designs the decision tree and the system executes it.

Most use cases don't need true agency. A well-prompted system with good tools can solve most problems. I think we should use agency where it adds real value, not everywhere.

This is part of why I'm excited about x402 and pay-per-tool. Real agents need to make economic decisions—when to spend money, which tools to use, how to budget. That's agency in action. Agents that can manage their own resource allocation are closer to true agency.

## Conclusion

I think agency is real, but it's rare. Most "agentic" systems are just well-prompted assistants, and that's okay—most problems don't need true agency. But when you do need it, it's important to build it right.

I think we're on the cusp of real agentic systems becoming practical. The infrastructure is getting better with MCP, x402, [ERC-8004](https://eips.ethereum.org/EIPS/eip-8004), and other protocols. But we need to use it thoughtfully and be honest about what we're building.

If you have thoughts on this or feel I missed something, I'd love to hear them. I'm still learning about this space myself.

import Footer from '../../snippets/_footer.mdx'

<Footer />
